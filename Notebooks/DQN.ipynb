{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DQN-Iter-1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "3wr6Rd2XRzWO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!apt-get install python-opengl -y\n",
        "!apt install xvfb -y\n",
        "!pip install pyvirtualdisplay"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2CxMs48R6Cc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "from IPython import display\n",
        "import matplotlib.pyplot as plt\n",
        "from pyvirtualdisplay import Display\n",
        "from collections import namedtuple\n",
        "import math\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Input,layers,Model\n",
        "\n",
        "display = Display(visible=0, size=(1400, 900)).start()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WIG6DhGQSBhS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# env_name = 'MountainCar-v0'\n",
        "env_name = 'CartPole-v0'\n",
        "env = gym.make(env_name)\n",
        "env.reset()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WSkKFwSoSCeC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learning_rate = 5e-4\n",
        "\n",
        "inputs = Input([*Env_info.input_dims])\n",
        "x = layers.Dense(256, activation = tf.nn.relu)(inputs)\n",
        "x = layers.Dense(256, activation = tf.nn.relu)(x)\n",
        "outputs = layers.Dense(Env_info.n_actions)(x)\n",
        "\n",
        "train_net = Model(inputs = inputs, outputs= outputs)\n",
        "target_net = tf.keras.models.clone_model(train_net)\n",
        "\n",
        "loss = tf.keras.losses.mean_squared_error\n",
        "opt = tf.keras.optimizers.Adam(learning_rate = learning_rate)\n",
        "\n",
        "train_net.compile(optimizer = opt, loss = loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "THnIZ719SC43",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Transition = namedtuple('Transition',\n",
        "                        ('observation', 'action', 'reward', 'next_observation', 'done'))\n",
        "\n",
        "class ReplayMemory:\n",
        "    def __init__(self, capacity):\n",
        "        self.capacity = capacity\n",
        "        self.memory = []\n",
        "        self.position = 0\n",
        "\n",
        "    def push(self, *args):\n",
        "        \"\"\"Saves a transition.\"\"\"\n",
        "        if len(self.memory) < self.capacity:\n",
        "            self.memory.append(None)\n",
        "        self.memory[self.position] = Transition(*args)\n",
        "        self.position = (self.position + 1) % self.capacity\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        return random.sample(self.memory, batch_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ScKXk2ZTSDEs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EPS_START = 1.0\n",
        "EPS_END = 0.0\n",
        "EPS_DECAY = 200\n",
        "\n",
        "class Epsilon:\n",
        "  eps = EPS_START\n",
        "  start = EPS_START\n",
        "  end = EPS_END\n",
        "  decay_factor = EPS_DECAY\n",
        "  steps_done = 0\n",
        "  @staticmethod\n",
        "  def decay():\n",
        "    Epsilon.eps = Epsilon.end + (Epsilon.start - Epsilon.end) * math.exp(-1.0 * (Epsilon.steps_done / Epsilon.decay_factor))\n",
        "    Epsilon.steps_done+=1\n",
        "    return Epsilon.eps\n",
        "\n",
        "# for i in range(2000):\n",
        "#   Epsilon.decay()\n",
        "# print(Epsilon.eps)  #4.562749785743468e-05"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IXEooud0SZRG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "UPDATE_EVERY = 10\n",
        "EXPERIENCE_MEMORY_SIZE = 100000\n",
        "GAMMA = 0.99 \n",
        "\n",
        "\n",
        "class Agent:\n",
        "  def __init__(self):\n",
        "    self.train_net = train_net\n",
        "    self.target_net = target_net\n",
        "    self.batch_size = BATCH_SIZE\n",
        "    self.t_step = 0\n",
        "    self.gamma = GAMMA\n",
        "    self.memory = ReplayMemory(EXPERIENCE_MEMORY_SIZE)\n",
        "    \n",
        "\n",
        "    self.losses = []\n",
        "    \n",
        "  \n",
        "  def predict(self, observation):\n",
        "    if random.random() > Epsilon.decay():\n",
        "      observation= tf.expand_dims(observation,axis = 0)\n",
        "      return tf.argmax(self.train_net(observation),axis=1).numpy()[0]\n",
        "    return env.action_space.sample()\n",
        "\n",
        "  def step(self, observation, action, reward, next_observation, done):\n",
        "    self.memory.push(observation, action, reward, next_observation, done)\n",
        "\n",
        "    # Learn every UPDATE_EVERY time steps.\n",
        "    self.t_step = (self.t_step + 1) % UPDATE_EVERY\n",
        "    if self.t_step == 0:\n",
        "      # If enough samples are available in memory, get random subset and learn\n",
        "      if len(self.memory) > BATCH_SIZE:\n",
        "        experiences = self.memory.sample(BATCH_SIZE)\n",
        "        self.train_step(experiences)\n",
        "        # loss_val = self.train_step(experiences)['loss']  #return loss value\n",
        "        # self.losses.append(loss_val)\n",
        "        # return loss_val \n",
        "\n",
        "\n",
        "  def train_step(self, transitions):\n",
        "    \n",
        "    # q_target = current_reward + gamma * q_future  # for usual cases\n",
        "    # if the next_state is terminal, then q_future for that particular state will become zero\n",
        "    # then it becomes q_target = current_reward\n",
        "    # this can be achieved by multiplying (1-done) \n",
        "\n",
        "    batch = Transition(*zip(*transitions))\n",
        "\n",
        "    current_observations = np.asarray(batch.observation)\n",
        "    next_observations = np.asarray(batch.next_observation)\n",
        "    rewards = np.asarray(batch.reward)\n",
        "    dones = np.asarray(batch.done)\n",
        "    actions = np.asarray(batch.action)\n",
        "    indices = tf.range(0,self.batch_size).numpy()\n",
        "\n",
        "    # print(current_observations.shape)\n",
        "    # print(next_observations.shape)\n",
        "    # print(rewards.shape)\n",
        "    # print(dones.shape)\n",
        "    # print(actions.shape)\n",
        "    # print(indices.shape)\n",
        "\n",
        "    q_vals = self.train_net(current_observations).numpy()\n",
        "    q_target = np.copy(q_vals)\n",
        "    q_next = tf.reduce_max(self.target_net(next_observations), axis = -1)\n",
        "    q_target[indices,actions] = rewards + self.gamma * q_next * (1.0-dones) \n",
        "\n",
        "    # print(\"Q-vals\")\n",
        "    # print(q_vals)\n",
        "    # print(\"Actions\")\n",
        "    # print(actions)\n",
        "    # print(\"Q-targets\")\n",
        "    # print(q_target)\n",
        "    # print(\"result\")\n",
        "    # print(q_vals-q_target)\n",
        "\n",
        "    #performs training and caches loss value in train_metrics\n",
        "    # loss_val = self.policy_net.train_on_batch(current_observations, q_target, return_dict = True)\n",
        "\n",
        "    #Low-level alternative to train_on_batch\n",
        "    with tf.GradientTape() as tape:\n",
        "      q_vals = self.train_net(current_observations)\n",
        "      loss_val = loss(q_vals, q_target)\n",
        "    gradients = tape.gradient(loss_val, self.train_net.trainable_variables)\n",
        "    opt.apply_gradients(zip(gradients, self.train_net.trainable_variables))\n",
        "\n",
        "    self.update_target_net()\n",
        "\n",
        "    \n",
        "  #Write soft_update\n",
        "  def update_target_net(self):\n",
        "    weights = self.train_net.get_weights()\n",
        "    self.target_net.set_weights(weights)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IVuKDBOSSjuh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import deque\n",
        "\n",
        "n_episodes = 2000\n",
        "log_every_n_episodes = 5\n",
        "\n",
        "scores = []\n",
        "scores_window = deque(maxlen = log_every_n_episodes)\n",
        "\n",
        "agent = Agent()\n",
        "\n",
        "for episode in range(n_episodes):\n",
        "\n",
        "  obs = env.reset()\n",
        "  score = 0\n",
        "  done = False\n",
        "\n",
        "  while not done:\n",
        "    action = agent.act(obs)\n",
        "    new_obs, reward, done, _ = env.step(action)\n",
        "    done = 1.0 if done else 0.0  # if done is True -> done = 1 else done = 0\n",
        "    agent.step(obs,action,reward,new_obs,done) \n",
        "    # loss_tmp = agent.step(obs,action,reward,new_obs,done) \n",
        "    # if loss_tmp == None: loss_tmp = 0\n",
        "    # loss += loss_tmp\n",
        "    score += reward\n",
        "    obs = new_obs\n",
        "  \n",
        "  scores.append(score)\n",
        "  scores_window.append(score)\n",
        "  \n",
        "  if episode% log_every_n_episodes == 0:\n",
        "    print('Episode {}  AverageScore: {:.2f}'.format(episode, np.mean(scores_window)))\n",
        "\n",
        "\n",
        "# Serializer(agent.target_net, )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wdwagW8gSq85",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import imageio\n",
        "import base64\n",
        "import IPython\n",
        "\n",
        "def embed_mp4(filename):\n",
        "  \"\"\"Embeds an mp4 file in the notebook.\"\"\"\n",
        "  video = open(filename,'rb').read()\n",
        "  b64 = base64.b64encode(video)\n",
        "  tag = '''\n",
        "  <video width=\"640\" height=\"480\" controls>\n",
        "    <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\">\n",
        "  Your browser does not support the video tag.\n",
        "  </video>'''.format(b64.decode())\n",
        "\n",
        "  return IPython.display.HTML(tag)\n",
        "\n",
        "\n",
        "def create_policy_eval_video(agent, filename, num_episodes=5, fps=30):\n",
        "  l = []\n",
        "  env_name = 'CartPole-v0'\n",
        "  env = gym.make(env_name)\n",
        "  filename = filename + \".mp4\"\n",
        "  with imageio.get_writer(filename, fps=fps) as video:\n",
        "    for _ in range(num_episodes):\n",
        "      steps = 0\n",
        "      obs = env.reset()\n",
        "      video.append_data(env.render(mode='rgb_array'))\n",
        "      done = False\n",
        "      while not done:\n",
        "        action = agent.act(obs)\n",
        "        new_obs,_,done,_ = env.step(action)\n",
        "        video.append_data(env.render(mode='rgb_array'))\n",
        "        obs = new_obs\n",
        "        steps+=1\n",
        "        if done:\n",
        "          l.append(steps)\n",
        "  print(l) \n",
        "  return embed_mp4(filename)\n",
        "agent = Agent()\n",
        "create_policy_eval_video(agent, \"trained-agent\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rXjBfDGmSsGG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}