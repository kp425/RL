{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Vanilla PolicyGradient.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Roi8kiFCU-qI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers,Model,Input\n",
        "import tensorflow_probability as tfp\n",
        "\n",
        "\n",
        "from collections import namedtuple, deque\n",
        "import numpy as np\n",
        "import gym\n",
        "\n",
        "# tf.compat.v1.disable_eager_execution()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2th69kJQVSyn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "b5d6ab80-747f-4ca7-ee5b-1377edf5e272"
      },
      "source": [
        "env_name = \"CartPole-v0\"\n",
        "env = gym.make(env_name)\n",
        "\n",
        "input_dims = env.observation_space.shape\n",
        "n_actions = env.action_space.n\n",
        "\n",
        "print(input_dims)\n",
        "print(n_actions)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(4,)\n",
            "2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D7y4YADFVOho",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learning_rate = 0.0001\n",
        "\n",
        "inputs = Input([*input_dims])\n",
        "x = layers.Dense(256, activation=tf.nn.relu)(inputs)\n",
        "x = layers.Dense(256, activation=tf.nn.relu)(x)\n",
        "outputs = layers.Dense(n_actions, activation = tf.nn.softmax)(x)\n",
        "\n",
        "train_net = Model(inputs = inputs, outputs = outputs)\n",
        "opt = tf.keras.optimizers.Adam(learning_rate = learning_rate)\n",
        "\n",
        "# train_net.compile(optimizer = opt)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YGzZrDMeXYmp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Agent:\n",
        "\n",
        "  def __init__(self):\n",
        "    self.train_net = train_net\n",
        "  \n",
        "  def control(self, action_log_probs, values_of_states, tape):\n",
        "    \n",
        "    action_log_probs = -1*tf.convert_to_tensor(action_log_probs)\n",
        "    values_of_states = tf.reshape(tf.convert_to_tensor(values_of_states),(-1,1))\n",
        "    loss_val = tf.reduce_sum(tf.multiply(action_log_probs, values_of_states))\n",
        "    gradients = tape.gradient(loss_val, self.train_net.trainable_variables)\n",
        "    opt.apply_gradients(zip(gradients, self.train_net.trainable_variables))\n",
        "    tape.__exit__(None,None,None)\n",
        "\n",
        "  @tf.function  \n",
        "  def predict(self, obs):\n",
        "    action_probs = self.train_net(tf.expand_dims(obs, axis=0))\n",
        "    m = tfp.distributions.Categorical(probs = action_probs)\n",
        "    action = m.sample()\n",
        "    return action, m.log_prob(action)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "73ZXHPfda7b5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "afd23de9-59ce-477e-8eb6-f177a8ce1fac"
      },
      "source": [
        "def reinforce(n_episodes=1500, max_iters=1000, gamma=1.0, print_every=100):\n",
        "\n",
        "  agent = Agent()\n",
        "  scores = []\n",
        "  scores_window = deque(maxlen = print_every)\n",
        "\n",
        "  for episode in range(n_episodes):\n",
        "    tape = tf.GradientTape()\n",
        "    tape.__enter__()\n",
        "    \n",
        "    score = 0\n",
        "    obs = env.reset()\n",
        "    done = False\n",
        "    saved_probs = []\n",
        "    rewards = []\n",
        "    for _ in range(max_iters):\n",
        "      action, action_log_prob = agent.predict(obs)\n",
        "      saved_probs.append(action_log_prob)\n",
        "      new_obs, reward, done, _ = env.step(action.numpy()[0])\n",
        "      rewards.append(reward)\n",
        "      score+=reward\n",
        "      obs = new_obs\n",
        "      if done:\n",
        "        break\n",
        "\n",
        "    scores.append(score)\n",
        "    scores_window.append(score)\n",
        "\n",
        "    discounts = [gamma**i for i in range(len(rewards)+1)]\n",
        "    R = sum([i*j for i,j in zip(discounts, rewards)]) # but why summing ? why not multiply correspoding elements of log probs with rewards ?\n",
        "    agent.control(saved_probs, R, tape)\n",
        "    \n",
        "\n",
        "\n",
        "  \n",
        "    if episode % print_every == 0:\n",
        "      print('Episode {}  AverageScore: {:.2f}'.format(episode, np.mean(scores_window)))\n",
        "\n",
        "reinforce()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Episode 0  AverageScore: 27.00\n",
            "Episode 100  AverageScore: 21.04\n",
            "Episode 200  AverageScore: 25.77\n",
            "Episode 300  AverageScore: 32.55\n",
            "Episode 400  AverageScore: 38.17\n",
            "Episode 500  AverageScore: 41.01\n",
            "Episode 600  AverageScore: 45.21\n",
            "Episode 700  AverageScore: 56.01\n",
            "Episode 800  AverageScore: 66.59\n",
            "Episode 900  AverageScore: 101.40\n",
            "Episode 1000  AverageScore: 108.41\n",
            "Episode 1100  AverageScore: 142.18\n",
            "Episode 1200  AverageScore: 118.58\n",
            "Episode 1300  AverageScore: 152.98\n",
            "Episode 1400  AverageScore: 180.32\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qBYfeee2t6r7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "d83796f5-ccfa-4595-e168-ed6c2000b69e"
      },
      "source": [
        "k = tf.random.uniform((10,))\n",
        "print(k)\n",
        "print(-1*k)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[0.02844322 0.47876954 0.91451204 0.7833332  0.67737305 0.38027084\n",
            " 0.7910849  0.67119    0.6244683  0.82444537], shape=(10,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[-0.02844322 -0.47876954 -0.91451204 -0.7833332  -0.67737305 -0.38027084\n",
            " -0.7910849  -0.67119    -0.6244683  -0.82444537], shape=(10,), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_BDeRTVJeK9H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-wPR25HoeLE7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yPs7Oyi0t6yI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "gym.logger.set_level(40) # suppress warnings (please remove if gives error)\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import torch\n",
        "torch.manual_seed(0) # set random seed\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Categorical\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZEseQGRwuBm-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "3d01be83-5422-41a1-f907-b449731e95e0"
      },
      "source": [
        "env = gym.make('CartPole-v0')\n",
        "env.seed(0)\n",
        "print('observation space:', env.observation_space)\n",
        "print('action space:', env.action_space)\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class Policy(nn.Module):\n",
        "    def __init__(self, s_size=4, h_size=16, a_size=2):\n",
        "        super(Policy, self).__init__()\n",
        "        self.fc1 = nn.Linear(s_size, h_size)\n",
        "        self.fc2 = nn.Linear(h_size, a_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return F.softmax(x, dim=1)\n",
        "    \n",
        "    def act(self, state):\n",
        "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
        "        probs = self.forward(state).cpu()\n",
        "        m = Categorical(probs)\n",
        "        action = m.sample()\n",
        "        return action.item(), m.log_prob(action)\n",
        "\n",
        "p = Policy()\n",
        "p.act(env.reset())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "observation space: Box(4,)\n",
            "action space: Discrete(2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, tensor([-0.5285], grad_fn=<SqueezeBackward1>))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aPjGenVauCl3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 963
        },
        "outputId": "17ed8f60-78cb-4638-f092-a8fa591ec2a0"
      },
      "source": [
        "policy = Policy().to(device)\n",
        "optimizer = optim.Adam(policy.parameters(), lr=1e-2)\n",
        "\n",
        "def reinforce(n_episodes=1000, max_t=1000, gamma=1.0, print_every=100):\n",
        "    scores_deque = deque(maxlen=100)\n",
        "    scores = []\n",
        "    for i_episode in range(1, n_episodes+1):\n",
        "        saved_log_probs = []\n",
        "        rewards = []\n",
        "        state = env.reset()\n",
        "        for t in range(max_t):\n",
        "            action, log_prob = policy.act(state)\n",
        "            saved_log_probs.append(log_prob)\n",
        "            state, reward, done, _ = env.step(action)\n",
        "            rewards.append(reward)\n",
        "            if done:\n",
        "                break \n",
        "        scores_deque.append(sum(rewards))\n",
        "        scores.append(sum(rewards))\n",
        "        \n",
        "        discounts = [gamma**i for i in range(len(rewards)+1)]\n",
        "        R = sum([a*b for a,b in zip(discounts, rewards)])\n",
        "        \n",
        "        policy_loss = []\n",
        "        for log_prob in saved_log_probs:\n",
        "            print(\"log_prob\", log_prob)\n",
        "            print(\"R\", R)\n",
        "            policy_loss.append(-log_prob * R)\n",
        "          \n",
        "       \n",
        "        \n",
        "        policy_loss = torch.cat(policy_loss)\n",
        "        print(policy_loss.shape)\n",
        "        policy_loss = policy_loss.sum()\n",
        "        print(policy_loss)\n",
        "        raise KeyboardInterrupt\n",
        "\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        policy_loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        if i_episode % print_every == 0:\n",
        "            print('Episode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_deque)))\n",
        "        if np.mean(scores_deque)>=195.0:\n",
        "            print('Environment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_deque)))\n",
        "            break\n",
        "        \n",
        "    return scores\n",
        "    \n",
        "scores = reinforce()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "log_prob tensor([-0.5922], grad_fn=<SqueezeBackward1>)\n",
            "R 18.0\n",
            "log_prob tensor([-0.6147], grad_fn=<SqueezeBackward1>)\n",
            "R 18.0\n",
            "log_prob tensor([-0.7654], grad_fn=<SqueezeBackward1>)\n",
            "R 18.0\n",
            "log_prob tensor([-0.7765], grad_fn=<SqueezeBackward1>)\n",
            "R 18.0\n",
            "log_prob tensor([-0.5955], grad_fn=<SqueezeBackward1>)\n",
            "R 18.0\n",
            "log_prob tensor([-0.7760], grad_fn=<SqueezeBackward1>)\n",
            "R 18.0\n",
            "log_prob tensor([-0.7996], grad_fn=<SqueezeBackward1>)\n",
            "R 18.0\n",
            "log_prob tensor([-0.5815], grad_fn=<SqueezeBackward1>)\n",
            "R 18.0\n",
            "log_prob tensor([-0.7989], grad_fn=<SqueezeBackward1>)\n",
            "R 18.0\n",
            "log_prob tensor([-0.5798], grad_fn=<SqueezeBackward1>)\n",
            "R 18.0\n",
            "log_prob tensor([-0.5983], grad_fn=<SqueezeBackward1>)\n",
            "R 18.0\n",
            "log_prob tensor([-0.6166], grad_fn=<SqueezeBackward1>)\n",
            "R 18.0\n",
            "log_prob tensor([-0.6316], grad_fn=<SqueezeBackward1>)\n",
            "R 18.0\n",
            "log_prob tensor([-0.7493], grad_fn=<SqueezeBackward1>)\n",
            "R 18.0\n",
            "log_prob tensor([-0.6321], grad_fn=<SqueezeBackward1>)\n",
            "R 18.0\n",
            "log_prob tensor([-0.6449], grad_fn=<SqueezeBackward1>)\n",
            "R 18.0\n",
            "log_prob tensor([-0.6634], grad_fn=<SqueezeBackward1>)\n",
            "R 18.0\n",
            "log_prob tensor([-0.7053], grad_fn=<SqueezeBackward1>)\n",
            "R 18.0\n",
            "torch.Size([18])\n",
            "tensor(218.1903, grad_fn=<SumBackward0>)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-09b23f01262c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreinforce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-12-09b23f01262c>\u001b[0m in \u001b[0;36mreinforce\u001b[0;34m(n_episodes, max_t, gamma, print_every)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mpolicy_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}