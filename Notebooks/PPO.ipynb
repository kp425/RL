{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PPO-Iter-6.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "O8LMvLeJgwNr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!apt-get install python-opengl\n",
        "!apt-get install swig cmake libopenmpi-dev zlib1g-dev xvfb x11-utils ffmpeg -qq \n",
        "!pip install box2d box2d-kengz pyvirtualdisplay pyglet --quiet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4GJMflX0SKx2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !git clone https://github.com/kp425/RL.git #clones master\n",
        "!git clone --branch Env-Branch https://github.com/kp425/RL.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2xNq2kYrZrST",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from RL.utils import timer, normalize\n",
        "from RL.env import SubprocVecEnv, VecFrameStack, make_env, make_atari_env\n",
        "from RL.GraphOps import Datapoint, live_plot\n",
        "from RL.policies.Policies import make_policy\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, Model, Input, Sequential\n",
        "import tensorflow_probability as tfp\n",
        "\n",
        "import os\n",
        "import gym\n",
        "import numpy as np\n",
        "from collections import deque, namedtuple\n",
        "from IPython.display import clear_output\n",
        "from pyvirtualdisplay import Display\n",
        "import matplotlib.pyplot as plt\n",
        "import PIL\n",
        "%matplotlib inline\n",
        "\n",
        "display = Display(visible=0, size=(1400, 900)).start()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8jrNFmWbDsSN",
        "colab_type": "text"
      },
      "source": [
        "# Setup an Env (Multi-processing wrapper)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FTD34fl5Z4cG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# env_name = \"MountainCarContinuous-v0\"\n",
        "env_name = \"CartPole-v0\"\n",
        "# env_name = \"Pendulum-v0\"\n",
        "\n",
        "num_envs = 8\n",
        "test_env = gym.make(env_name)\n",
        "envs = [make_env(env_name) for i in range(num_envs)]\n",
        "env = SubprocVecEnv(envs)\n",
        "# save_path = \"drive/My Drive/Colab Notebooks/SeriousRL/Trained/{}\".format(env_name)\n",
        "save_path = \"RL/Trained/{}\".format(env_name)\n",
        "# save_path = None\n",
        "\n",
        "\n",
        "#Atari specific\n",
        "\n",
        "# env_name = \"Assault-v0\"\n",
        "# num_envs = 2\n",
        "# env = gym.make(env_name)\n",
        "# test_env = gym.make(env_name)\n",
        "\n",
        "# envs = [make_atari_env(env_name) for i in range(num_envs)]\n",
        "# env = SubprocVecEnv(envs)\n",
        "# # env = VecFrameStack(env, 4)\n",
        "\n",
        "# print(env.reset().shape)\n",
        "\n",
        "# save_path = \"drive/My Drive/Colab Notebooks/SeriousRL/Trained/{}\".format(env_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Le1bUuErEB_z",
        "colab_type": "text"
      },
      "source": [
        "# Setting up Function approximator and Policy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "14SKf_kHP3ZU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "outputId": "ec4bf619-96d7-47b9-fb0a-10b0e3844df2"
      },
      "source": [
        "def mlp_net(input_shape, n_outputs):\n",
        "    \n",
        "    inputs = Input(shape = input_shape)\n",
        "    \n",
        "    policy_layers = [layers.Dense(128, activation = tf.nn.relu, name = \"policy_layers\")]\n",
        "    p_inputs = inputs\n",
        "    for p_layer in policy_layers:\n",
        "        p_inputs = p_layer(p_inputs)\n",
        "\n",
        "    value_layers = [layers.Dense(128, activation = tf.nn.relu, name = \"value_layers\")]\n",
        "    v_inputs = inputs\n",
        "    for v_layer in value_layers:\n",
        "        v_inputs = v_layer(v_inputs)\n",
        "    \n",
        "    policy_head = layers.Dense(n_outputs, activation = tf.nn.softmax, name = \"policy_head\")(p_inputs) \n",
        "    value_head = layers.Dense(1, activation = tf.nn.tanh, name = \"value_head\")(v_inputs)\n",
        "    model = Model(inputs = [inputs], outputs = [policy_head, value_head])\n",
        "\n",
        "    return model\n",
        "\n",
        "lr = 0.0001\n",
        "model = make_policy(env.observation_space, env.action_space, net = mlp_net, save_path=save_path)\n",
        "model.get_architecture()\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate = lr)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using default net...\n",
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_3 (InputLayer)            [(None, 4)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "policy_layers (Dense)           (None, 128)          640         input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "value_layers (Dense)            (None, 128)          640         input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "policy_head (Dense)             (None, 2)            258         policy_layers[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "value_head (Dense)              (None, 1)            129         value_layers[0][0]               \n",
            "==================================================================================================\n",
            "Total params: 1,667\n",
            "Trainable params: 1,667\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "575RNK1zELw2",
        "colab_type": "text"
      },
      "source": [
        "# Compute returns using technique \"Generalized advantage Estimation\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4g6VwYkaSTR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test(n_episodes = 5):\n",
        "    scores = []\n",
        "    for episode in range(n_episodes):\n",
        "        state = test_env.reset()\n",
        "        done = False\n",
        "        score = 0\n",
        "        while not done:\n",
        "            action, _,_ = model(state)\n",
        "            next_state, reward, done, _ = test_env.step(action.numpy()[0])\n",
        "            score += reward\n",
        "            state = next_state\n",
        "        scores.append(score)\n",
        "    print(scores)\n",
        "    avg_score = sum(scores)/len(scores)\n",
        "    return avg_score\n",
        "\n",
        "def compute_returns_with_gae(next_state, rewards, values, masks):\n",
        "    returns = []\n",
        "    values = values + [np.squeeze(model(next_state)[2])] # we do this to avoid index out of error\n",
        "    gae = 0\n",
        "    for i in reversed(range(len(rewards))):\n",
        "        delta = (rewards[i] + gamma* values[i+1]* masks[i]) - values[i]\n",
        "        gae = delta + gamma * smoothing_factor * gae * masks[i]\n",
        "        returns.insert(0, gae + values[i])\n",
        "    return returns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "id30Qh0aE-Ev",
        "colab_type": "text"
      },
      "source": [
        "# Training PPO"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qvCLP8bQaZF2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CLIP_RATIOS = 0.2\n",
        "CLIP_VALUES = 0.0\n",
        "gamma = 0.99\n",
        "PPO_STEPS = 128\n",
        "MINI_BATCH_SIZE = 4\n",
        "PPO_EPOCHS = 4\n",
        "smoothing_factor = 0.95\n",
        "\n",
        "\n",
        "policy_dp = Datapoint(\"policy_loss\")\n",
        "value_dp = Datapoint(\"value_loss\")\n",
        "entropy_dp = Datapoint(\"entropy_loss\")\n",
        "loss_dp = Datapoint(\"loss_dp\")\n",
        "train_step_counter = 0\n",
        "\n",
        "policy_dp_avg = Datapoint(\"policy_loss_avg\")\n",
        "value_dp_avg = Datapoint(\"value_loss_avg\")\n",
        "entropy_dp_avg = Datapoint(\"entropy_loss_avg\")\n",
        "loss_dp_avg = Datapoint(\"loss_dp_avg\")\n",
        "test_dp = Datapoint(\"test\")\n",
        "\n",
        "class DataPerEpoch:\n",
        "    policy_loss = 0.0\n",
        "    entropy_loss = 0.0\n",
        "    value_loss = 0.0\n",
        "    loss = 0.0\n",
        "\n",
        "\n",
        "\n",
        "def make_dataset(*args, n_samples = PPO_STEPS, batch_size = MINI_BATCH_SIZE):\n",
        "    pack = []\n",
        "    for i in args:\n",
        "        pack.append(tf.data.Dataset.from_tensor_slices(i))\n",
        "    dataset = tf.data.Dataset.zip((*pack))\n",
        "    dataset = dataset.shuffle(n_samples, reshuffle_each_iteration= True).repeat(PPO_EPOCHS)\n",
        "    dataset = dataset.batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "    return dataset\n",
        "\n",
        "\n",
        "def ppo_collect(sample_size = 50, gamma = gamma, smoothing_factor = smoothing_factor):\n",
        "    log_probs = []\n",
        "    values    = []\n",
        "    states    = []\n",
        "    actions   = []\n",
        "    rewards   = []\n",
        "    masks     = []\n",
        "\n",
        "    state = env.reset()  # Try resetting to random observation\n",
        "    for _ in range(sample_size):\n",
        "\n",
        "        action, dist, value = model(state)\n",
        "        log_prob = dist.log_prob(action)\n",
        "        next_state, reward, done, _ = env.step(action.numpy())\n",
        "  \n",
        "        log_probs.append(log_prob)\n",
        "        values.append(np.squeeze(value))\n",
        "        states.append(state)\n",
        "        actions.append(action)\n",
        "        rewards.append(reward)\n",
        "        masks.append(1.0-done)\n",
        "        state = next_state\n",
        "\n",
        "    returns = compute_returns_with_gae(next_state, rewards, values, masks)\n",
        "    \n",
        "    states = tf.concat(states, axis = 0)\n",
        "    actions = tf.squeeze(tf.concat(actions, axis = 0))\n",
        "    log_probs = tf.squeeze(tf.concat(log_probs, axis = 0))\n",
        "    values = tf.concat(values, axis = 0)\n",
        "    returns = tf.cast(tf.concat(returns, axis = 0), dtype = tf.float32)\n",
        "    advantages = tf.math.subtract(returns, values)\n",
        "  \n",
        "    n = sample_size * num_envs\n",
        "\n",
        "    tf.debugging.assert_shapes([(states, (n, * env.observation_space.shape)),\n",
        "                                (actions, (n,)),\n",
        "                                (log_probs, (n,)),\n",
        "                                (values, (n,)),\n",
        "                                (returns, (n,)),\n",
        "                                (advantages, (n,))])\n",
        "\n",
        "    # tf.debugging.assert_shapes([(states, (n, None)),\n",
        "    #                             (actions, (n,)),\n",
        "    #                             (log_probs, (n,)),\n",
        "    #                             (values, (n,)),\n",
        "    #                             (returns, (n,)),\n",
        "    #                             (advantages, (n,))])\n",
        "    \n",
        "    return states, actions, log_probs, values, returns, advantages\n",
        "    \n",
        "\n",
        "def policy_loss_fn(dist, actions, old_log_probs, advantages, clip = CLIP_RATIOS):\n",
        "\n",
        "    with tf.name_scope(name = \"policy_loss\"):\n",
        "        new_log_probs = tf.squeeze(dist.log_prob(actions))\n",
        "        ratios = tf.exp(new_log_probs - old_log_probs, name = \"ratios\")        \n",
        "        surr1 = tf.math.multiply(ratios, advantages, name = \"surr1\")\n",
        "        if clip > 0.0:\n",
        "            clipped_ratios = tf.clip_by_value(ratios, 1.0-clip, 1.0+clip, name = \"clipped_ratios\")\n",
        "            surr2 = tf.math.multiply(clipped_ratios, advantages, name = \"surr2\")\n",
        "            policy_loss = -tf.reduce_mean(tf.math.minimum(surr1, surr2))\n",
        "        else:\n",
        "            policy_loss = -tf.reduce_mean(surr1)\n",
        "\n",
        "        n = MINI_BATCH_SIZE\n",
        "        tf.debugging.assert_shapes([(new_log_probs, (n,)),\n",
        "                                (ratios, (n,)),\n",
        "                                (surr1, (n,)),\n",
        "                                (policy_loss,(1,))])\n",
        "        if clip > 0.0:\n",
        "            tf.debugging.assert_shapes([(clipped_ratios, (n,)),\n",
        "                                (surr2, (n,))])\n",
        "\n",
        "    DataPerEpoch.policy_loss = policy_loss\n",
        "    return policy_loss\n",
        "\n",
        "\n",
        "def approx_kl(old_log_probs, new_log_probs):\n",
        "    return .5 * tf.reduce_mean(tf.square(neglogpac - self.old_neglog_pac_ph))\n",
        "\n",
        "\n",
        "def value_loss_fn(values, old_values, returns, clip = CLIP_VALUES):\n",
        "    with tf.name_scope(name = \"value_loss\"):\n",
        "        values = tf.squeeze(values, name = \"values\")\n",
        "        value_error = tf.math.squared_difference(returns, values)\n",
        "        if clip > 0.0:\n",
        "            clipped_values = old_values + tf.clip_by_value(values - old_values, -clip, clip)\n",
        "            clipped_value_error = tf.math.squared_difference(returns, clipped_values)\n",
        "            value_loss = tf.reduce_mean(tf.math.maximum(value_error, clipped_value_error))\n",
        "        else:\n",
        "            value_loss = tf.reduce_mean(value_error)\n",
        "    \n",
        "    n = MINI_BATCH_SIZE\n",
        "    tf.debugging.assert_shapes([(values, (n,)),\n",
        "                                (value_error, (n,)),\n",
        "                                (value_loss, (1,))])\n",
        "    if clip > 0.0:\n",
        "        tf.debugging.assert_shapes([(clipped_values, (n,)),\n",
        "                                    (clipped_value_error, (n,))])\n",
        "                        \n",
        "    DataPerEpoch.value_loss = value_loss\n",
        "\n",
        "    return value_loss\n",
        "\n",
        "def entropy_loss_fn(dist):\n",
        "    with tf.name_scope(name = \"entropy_loss\"):\n",
        "        entropy_loss = tf.reduce_mean(dist.entropy())\n",
        "    DataPerEpoch.entropy_loss = entropy_loss\n",
        "    return entropy_loss\n",
        "\n",
        "\n",
        "def loss_fn(states, actions, old_log_probs, old_values, returns, advantages, \n",
        "            clip_ratios = CLIP_RATIOS, clip_values = CLIP_VALUES):\n",
        "    _, dist, values = model(states)\n",
        "    dist = tfp.distributions.BatchReshape(dist, [dist.batch_shape[0]])\n",
        "\n",
        "    with tf.name_scope(name = \"loss\"):\n",
        "        policy_loss = policy_loss_fn(dist, actions, old_log_probs, advantages, clip = CLIP_RATIOS)\n",
        "        value_loss = value_loss_fn(values, old_values, returns, clip = CLIP_VALUES)\n",
        "        entropy_loss = entropy_loss_fn(dist)\n",
        "        loss = policy_loss + 0.5*value_loss - 0.001 * entropy_loss\n",
        "    DataPerEpoch.loss = loss\n",
        "    return loss\n",
        "\n",
        "\n",
        "def train_step(batch):\n",
        "    with tf.GradientTape() as tape:\n",
        "        loss = loss_fn(*batch)\n",
        "    with tf.name_scope(name = \"calculating_grads\"):\n",
        "        grads = tape.gradient(loss, model.trainable_variables)\n",
        "    with tf.name_scope(name = \"applying_optimizer\"):\n",
        "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "\n",
        "\n",
        "def train(max_steps = 10000, collect_data= True, report_every = 250, save_every = 5000):\n",
        "\n",
        "    train_step_counter = 1\n",
        "    threshold_reward = 195.0\n",
        "    \n",
        "    while train_step_counter < max_steps:\n",
        "        \n",
        "        data = ppo_collect(sample_size = PPO_STEPS)\n",
        "        dataset = make_dataset(data)\n",
        "        for batch in dataset:\n",
        "            train_step(batch)\n",
        "            if collect_data == True:\n",
        "                policy_dp.collect(train_step_counter, DataPerEpoch.policy_loss.numpy())\n",
        "                value_dp.collect(train_step_counter, DataPerEpoch.value_loss.numpy())\n",
        "                entropy_dp.collect(train_step_counter, DataPerEpoch.entropy_loss.numpy())\n",
        "                loss_dp.collect(train_step_counter, DataPerEpoch.loss.numpy())\n",
        "            \n",
        "            if train_step_counter % report_every == 0:\n",
        "                from_index = ((train_step_counter // report_every) - 1)*report_every\n",
        "                policy_dp_avg.collect(train_step_counter, policy_dp.avg_y(from_index = from_index))\n",
        "                value_dp_avg.collect(train_step_counter, value_dp.avg_y(from_index = from_index))\n",
        "                entropy_dp_avg.collect(train_step_counter, entropy_dp.avg_y(from_index = from_index))\n",
        "                loss_dp_avg.collect(train_step_counter, loss_dp.avg_y(from_index = from_index))\n",
        "                # test_reward = test()\n",
        "                test_reward = test()\n",
        "                test_dp.collect(train_step_counter, test_reward)\n",
        "                live_plot([policy_dp_avg, value_dp_avg, entropy_dp_avg, loss_dp_avg, test_dp])\n",
        "                if test_reward > threshold_reward: \n",
        "                    print(\"Saving....\")\n",
        "                    model.save()\n",
        "                    return\n",
        "        \n",
        "            if train_step_counter % save_every == 0:\n",
        "                print(\"Saving....\")\n",
        "                model.save()    \n",
        "            train_step_counter += 1\n",
        "\n",
        "        \n",
        "\n",
        "train(max_steps = 25000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37sSsMlPFDuv",
        "colab_type": "text"
      },
      "source": [
        "# Makes Video using the trained policy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3AuWxSVxCeH6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import os\n",
        "# os.system(\"Xvfb :1 -screen 0 1024x768x24 &\")\n",
        "# os.environ['DISPLAY'] = ':1'\n",
        "\n",
        "import imageio\n",
        "import base64\n",
        "import IPython\n",
        "\n",
        "def embed_mp4(filename):\n",
        "  \"\"\"Embeds an mp4 file in the notebook.\"\"\"\n",
        "  video = open(filename,'rb').read()\n",
        "  b64 = base64.b64encode(video)\n",
        "  tag = '''\n",
        "  <video width=\"640\" height=\"480\" controls>\n",
        "    <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\">\n",
        "  Your browser does not support the video tag.\n",
        "  </video>'''.format(b64.decode())\n",
        "\n",
        "  return IPython.display.HTML(tag)\n",
        "\n",
        "\n",
        "def create_policy_eval_video(agent, env_name, directory, filename, num_episodes=1, fps=30):\n",
        "    if not os.path.exists(save_path):\n",
        "        os.makedirs(save_path)\n",
        "    video_env = gym.make(env_name)\n",
        "    filename = os.path.join(save_path, filename + \".mp4\")\n",
        "    with imageio.get_writer(filename, fps=fps) as video:\n",
        "        for _ in range(num_episodes):\n",
        "            state = video_env.reset()\n",
        "            video.append_data(video_env.render(mode='rgb_array'))\n",
        "            done = False\n",
        "            while not done:\n",
        "                action,_,_ = model(state)\n",
        "                new_state,_,done,_ = video_env.step(action.numpy()[0])\n",
        "                video.append_data(video_env.render(mode='rgb_array'))\n",
        "                state = new_state\n",
        "    return embed_mp4(filename)\n",
        "\n",
        "# env_name = \"CartPole-v0\"\n",
        "create_policy_eval_video(model, env_name, save_path, \"trained-agent\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}